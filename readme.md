# Spark Distributed Cluster (Standalone Mode)

Ce projet dÃ©ploie un **cluster Apache Spark** en mode *standalone*, entiÃ¨rement conteneurisÃ© avec **Docker**.  
Il permet dâ€™exÃ©cuter des applications PySpark en distribuant la charge sur plusieurs machines du mÃªme rÃ©seau local.

## ğŸ“Œ Architecture du cluster

- **1 Spark Master**
- **1 ou plusieurs Spark Workers**
- **1 Driver Python** pour exÃ©cuter les applications PySpark
- PossibilitÃ© dâ€™ajouter facilement des workers sur dâ€™autres machines du rÃ©seau (Wi-Fi ou Ethernet)

Le script principal charge le dataset **KDD Cup**, effectue des transformations et gÃ©nÃ¨re un graphique de scaling.

---

## ğŸ› ï¸ PrÃ©requis

- **Docker** installÃ© sur toutes les machines
- Toutes les machines connectÃ©es au **mÃªme rÃ©seau local**
- Dataset **KDD Cup** disponible dans le projet ou rÃ©cupÃ©rÃ© via script

---

## ğŸš€ Construction et lancement du cluster

### 1. Builder les images Docker

```bash
docker build -t spark-master -f Dockerfile.master .
docker build -t spark-worker -f Dockerfile.worker .
docker build -t spark-driver -f Dockerfile.driver .
./start-master.sh
./start-worker.sh
./start-driver.sh
```